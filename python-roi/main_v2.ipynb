{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059946e4",
   "metadata": {},
   "source": [
    "# Time Series problem: Predvidjanje kretanja prodaja u zavistonsti od razlicitih scenarija\n",
    "\n",
    "- **Trend:** `flat` (neutral), `up` (rising), `down` (falling)  \n",
    "- **Seasonality:** `none`, `monthly`, `yearly`, `monthly+yearly`\n",
    "\n",
    "U obzir su uzeti i neradni dani u Srbiji u skadu sa zakonom\n",
    "Takodje postoje i scenariji sa nepravilnim unosom podataka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcc370",
   "metadata": {},
   "source": [
    "## 1) Import biblioteka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea9bfb",
   "metadata": {},
   "source": [
    "## 2) Generisanje neradnih dana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthodox_easter_gregorian(year: int) -> date:\n",
    "    \"\"\"Proracun za pravoslavni uskrs, posto se menja svake godine\"\"\"\n",
    "    a = year % 4\n",
    "    b = year % 7\n",
    "    c = year % 19\n",
    "    d = (19 * c + 15) % 30\n",
    "    e = (2 * a + 4 * b - d + 34) % 7\n",
    "    month = (d + e + 114) // 31\n",
    "    day_ = ((d + e + 114) % 31) + 1\n",
    "    julian_easter = date(year, month, day_)\n",
    "    return julian_easter + timedelta(days=13)  # Julian -> Gregorian for 1900–2099\n",
    "\n",
    "def serbia_public_holidays(year: int) -> dict[date, str]:\n",
    "    \"\"\"Drzavni praznici zajedno sa uskrsom\"\"\"\n",
    "    holidays = {\n",
    "        date(year, 1, 1): \"New Year's Day\",\n",
    "        date(year, 1, 2): \"New Year's Day\",\n",
    "        date(year, 1, 7): \"Julian Orthodox Christmas\",\n",
    "        date(year, 2, 15): \"Statehood Day\",\n",
    "        date(year, 2, 16): \"Statehood Day\",\n",
    "        date(year, 5, 1): \"Labour Day\",\n",
    "        date(year, 5, 2): \"Labour Day\",\n",
    "        date(year, 11, 11): \"Armistice Day\",\n",
    "    }\n",
    "\n",
    "    easter = orthodox_easter_gregorian(year)\n",
    "    holidays.update({\n",
    "        easter + timedelta(days=-2): \"Orthodox Good Friday\",\n",
    "        easter + timedelta(days=-1): \"Orthodox Holy Saturday\",\n",
    "        easter + timedelta(days=0): \"Orthodox Easter Sunday\",\n",
    "        easter + timedelta(days=1): \"Orthodox Easter Monday\",\n",
    "    })\n",
    "\n",
    "    # Zakon: Ako jedan od datuma kada se praznuju državni praznici Republike Srbije padne u nedelju, ne radi se prvog narednog radnog dana.\n",
    "    state_holidays = {\"New Year's Day\", \"Statehood Day\", \"Labour Day\", \"Armistice Day\"}\n",
    "    all_dates = set(holidays.keys())\n",
    "\n",
    "    non_rel_dates = [d for d, name in holidays.items() if name in state_holidays]\n",
    "    for d in sorted(non_rel_dates):\n",
    "        if d.weekday() == 6:  # Sunday\n",
    "            obs = d + timedelta(days=1)\n",
    "            while obs.weekday() >= 5 or obs in all_dates:\n",
    "                obs += timedelta(days=1)\n",
    "            holidays[obs] = holidays[d] + \" (observed)\"\n",
    "            all_dates.add(obs)\n",
    "\n",
    "    return holidays\n",
    "\n",
    "def serbia_holiday_map_for_range(start: str, end: str) -> dict[pd.Timestamp, str]:\n",
    "    \"\"\"Return {Timestamp: name} for all holidays between start and end (inclusive).\"\"\"\n",
    "    start_dt = pd.Timestamp(start).date()\n",
    "    end_dt = pd.Timestamp(end).date()\n",
    "    years = range(start_dt.year, end_dt.year + 1)\n",
    "    out = {}\n",
    "    for y in years:\n",
    "        for d, name in serbia_public_holidays(y).items():\n",
    "            if start_dt <= d <= end_dt:\n",
    "                out[pd.Timestamp(d)] = name\n",
    "    return out\n",
    "\n",
    "# Quick demo: list holidays in 2026\n",
    "demo = serbia_public_holidays(2026)\n",
    "sorted(list(demo.items()))[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae0943",
   "metadata": {},
   "source": [
    "## 3) Generisanje podataka na osnovu trendova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa841a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_daily_sales(\n",
    "    start: str = \"2023-01-01\",\n",
    "    periods: int = 730,\n",
    "    base_level: float = 30.0,\n",
    "    trend: str = \"flat\",                 # 'flat' | 'up' | 'down'\n",
    "    seasonality: str = \"monthly+yearly\", # 'none' | 'monthly' | 'yearly' | 'monthly+yearly'\n",
    "    weekly_pattern: bool = True,\n",
    "\n",
    "    close_sundays: bool = True,\n",
    "    close_saturdays: bool = False,\n",
    "    close_on_public_holidays: bool = True,\n",
    "\n",
    "    # Noise / outliers\n",
    "    noise_scale: float = 0.20,           # multiplicative noise strength\n",
    "    spike_prob: float = 0.02,            # random spikes (outliers)\n",
    "    spike_mult_range: tuple = (1.1, 2.3),\n",
    "    overdispersion_k: float = 12.0,      # lower => more variance; higher => more Poisson-like\n",
    "\n",
    "    # BAD USER INPUT SCENARIO:\n",
    "    # user enters weekly totals only once per week (at end of week, not necessarily Friday)\n",
    "    weekly_batch_input: bool = False,\n",
    "    weekly_batch_last_k_days: int = 3,   # choose the entry day among the last K days of each week\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If weekly_batch_input=True:\n",
    "      - We still generate a realistic *true* daily series (sales_true)\n",
    "      - Then we convert the observed 'sales' into a once-per-week lump:\n",
    "        the week sum is entered on a random day among the last K days of the week,\n",
    "        and other days are set to 0.\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(start=start, periods=periods, freq=\"D\")\n",
    "    t = np.arange(periods, dtype=float)\n",
    "\n",
    "    # Trend (smooth)\n",
    "    if trend == \"flat\":\n",
    "        growth_rate = 0.00\n",
    "    elif trend == \"up\":\n",
    "        growth_rate = 0.35\n",
    "    elif trend == \"down\":\n",
    "        growth_rate = -0.25\n",
    "    else:\n",
    "        raise ValueError(\"trend must be one of: 'flat', 'up', 'down'\")\n",
    "\n",
    "    level = base_level * np.power(1.0 + growth_rate, t / 365.25)\n",
    "    level = np.maximum(1.0, level)\n",
    "\n",
    "    # Monthly/Yearly seasonality (additive)\n",
    "    seasonal = np.zeros_like(t)\n",
    "    if seasonality in (\"monthly\", \"monthly+yearly\"):\n",
    "        seasonal += 0.25 * base_level * np.sin(2 * np.pi * t / 30.4375)\n",
    "    if seasonality in (\"yearly\", \"monthly+yearly\"):\n",
    "        seasonal += 0.40 * base_level * np.sin(2 * np.pi * t / 365.25)\n",
    "\n",
    "    # Weekly pattern (multiplicative)\n",
    "    if weekly_pattern:\n",
    "        dow = dates.dayofweek.values # vraca niz od vrednosti dana [0-6]\n",
    "        # Mon..Sun multipliers (example pattern)\n",
    "        dow_mult = np.array([1.00, 1.03, 1.05, 1.02, 1.12, 1.18, 0.78])\n",
    "        weekly_mult = dow_mult[dow]\n",
    "    else:\n",
    "        weekly_mult = 1.0\n",
    "\n",
    "    # Serbia public holidays\n",
    "    hol_map = serbia_holiday_map_for_range(dates.min().strftime(\"%Y-%m-%d\"), dates.max().strftime(\"%Y-%m-%d\"))\n",
    "    holiday_name = pd.Series(index=dates, data=[hol_map.get(d.normalize(), \"\") for d in dates], dtype=\"string\")\n",
    "    is_holiday = holiday_name.ne(\"\") # creates bool array\n",
    "\n",
    "    # Closed policy\n",
    "    is_weekend = (dates.dayofweek.values >= 5)\n",
    "    is_closed = np.zeros(periods, dtype=bool)\n",
    "    # |= je kao  is_closed = is_closed OR (dates.dayofweek.values == 5)\n",
    "    if close_saturdays:\n",
    "        is_closed |= (dates.dayofweek.values == 5)\n",
    "    if close_sundays:\n",
    "        is_closed |= (dates.dayofweek.values == 6)\n",
    "    if close_on_public_holidays:\n",
    "        is_closed |= is_holiday.values\n",
    "\n",
    "    # Expected mean (before spikes/noise)\n",
    "    mu = (level + seasonal) * weekly_mult\n",
    "    mu = np.maximum(0.1, mu)\n",
    "\n",
    "    # Random spikes (not on closed days)\n",
    "    spike = (np.random.rand(periods) < spike_prob) & (~is_closed)\n",
    "    spike_mult = np.ones(periods)\n",
    "    spike_mult[spike] = np.random.uniform(spike_mult_range[0], spike_mult_range[1], size=int(spike.sum()))\n",
    "    mu *= spike_mult\n",
    "\n",
    "    # Multiplicative noise\n",
    "    eps = np.random.normal(0.0, noise_scale, size=periods)\n",
    "    mu_noisy = mu * np.exp(eps)\n",
    "    mu_noisy = np.maximum(0.0, mu_noisy)\n",
    "\n",
    "    # Force closed days to zero demand\n",
    "    mu_noisy[is_closed] = 0.0\n",
    "\n",
    "    # Sample counts via Negative Binomial (extra variance)\n",
    "    k = float(overdispersion_k) # larger k value gives smoother results\n",
    "    sales = np.zeros(periods, dtype=int)\n",
    "    nz = mu_noisy > 0\n",
    "    mu_nz = mu_noisy[nz]\n",
    "    p = k / (k + mu_nz)\n",
    "    sales[nz] = np.random.negative_binomial(n=k, p=p, size=mu_nz.shape[0]).astype(int)\n",
    "\n",
    "    # Keep the true daily signal\n",
    "    sales_true = sales.copy()\n",
    "    weekly_entry = np.zeros(periods, dtype=bool)\n",
    "\n",
    "    # BAD INPUT: weekly batch entry at end of week (random day among last K days)\n",
    "    if weekly_batch_input:\n",
    "        sales_obs = np.zeros_like(sales_true)\n",
    "        # Week from Monday..Sunday (ends Sunday)\n",
    "        wk = dates.to_period(\"W-SUN\")\n",
    "        for w in pd.unique(wk):\n",
    "            idx = np.where(wk == w)[0]\n",
    "            if idx.size == 0:\n",
    "                continue\n",
    "            week_total = int(sales_true[idx].sum())\n",
    "\n",
    "            # choose among last K days in the week (or fewer if week is short at edges)\n",
    "            k_days = int(max(1, weekly_batch_last_k_days))\n",
    "            candidates = idx[-k_days:]\n",
    "            chosen = int(np.random.choice(candidates))\n",
    "\n",
    "            sales_obs[chosen] = week_total\n",
    "            weekly_entry[chosen] = True\n",
    "\n",
    "        sales = sales_obs\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"sales\": sales,                 # observed sales (may be weekly-batched)\n",
    "        \"sales_true\": sales_true,       # underlying true daily sales\n",
    "        \"mu\": mu_noisy,                 # expected mean (daily)\n",
    "        \"is_weekend\": is_weekend,\n",
    "        \"is_holiday\": is_holiday.values,\n",
    "        \"holiday_name\": holiday_name.values,\n",
    "        \"is_closed\": is_closed,\n",
    "        \"spike\": spike,\n",
    "        \"weekly_entry\": weekly_entry,   # True on the day user entered weekly total (bad input scenario)\n",
    "        \"weekly_batch_input\": weekly_batch_input,\n",
    "    })\n",
    "\n",
    "# Smoke tests\n",
    "generate_daily_sales(trend=\"up\", seasonality=\"monthly+yearly\", periods=30, weekly_batch_input=False).head(),\n",
    "generate_daily_sales(trend=\"up\", seasonality=\"monthly+yearly\", periods=30, weekly_batch_input=True).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34aa3aa",
   "metadata": {},
   "source": [
    "## 4) Kreiranje scenarija (3 trends x 4 seasonalities = 12 clean series + 1 bad user input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5965a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRENDS = [\"flat\", \"up\", \"down\"]\n",
    "SEASONALITIES = [\"none\", \"monthly\", \"yearly\", \"monthly+yearly\"]\n",
    "\n",
    "scenarios = {}\n",
    "for tr in TRENDS:\n",
    "    for seas in SEASONALITIES:\n",
    "        key = f\"{tr}__{seas}\"\n",
    "        scenarios[key] = generate_daily_sales(\n",
    "            start=\"2025-01-01\",\n",
    "            periods=3 * 365,\n",
    "            base_level=35.0,\n",
    "            trend=tr,\n",
    "            seasonality=seas,\n",
    "            weekly_pattern=True,\n",
    "            close_sundays=True,\n",
    "            close_saturdays=False,\n",
    "            close_on_public_holidays=True,\n",
    "            noise_scale=0.18,\n",
    "            spike_prob=0.02,\n",
    "            overdispersion_k=10.0,\n",
    "            weekly_batch_input=False,\n",
    "        )\n",
    "\n",
    "# Single \"bad user input\" scenario: weekly batch entry at end of week\n",
    "scenarios[\"bad_user_input\"] = generate_daily_sales(\n",
    "    start=\"2025-01-01\",\n",
    "    periods=3 * 365,\n",
    "    base_level=35.0,\n",
    "    trend=\"flat\",\n",
    "    seasonality=\"monthly+yearly\",\n",
    "    weekly_pattern=True,\n",
    "    close_sundays=True,\n",
    "    close_saturdays=False,\n",
    "    close_on_public_holidays=True,\n",
    "    noise_scale=0.18,\n",
    "    spike_prob=0.02,\n",
    "    overdispersion_k=10.0,\n",
    "    weekly_batch_input=True,\n",
    "    weekly_batch_last_k_days=3,   # entry day among last 3 days of week (not necessarily Friday)\n",
    ")\n",
    "\n",
    "list(scenarios.keys())[:], len(scenarios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961f30b",
   "metadata": {},
   "source": [
    "## 4b) Cuvanje podataka u CSV fajlove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"Data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# After creating full datasets, also export fixed-length subsets for benchmarking\n",
    "SUBSET_DAYS = [30, 100, 300]     # you can add more here\n",
    "SUBSET_ANCHOR = \"last\"           # 'last' or 'first'\n",
    "\n",
    "saved = 0\n",
    "saved_sub = 0\n",
    "\n",
    "for name, df_s in scenarios.items():\n",
    "    # Full dataset\n",
    "    df_s.to_csv(out_dir / f\"{name}.csv\", index=False)\n",
    "    saved += 1\n",
    "\n",
    "    # Subsets\n",
    "    for n in SUBSET_DAYS:\n",
    "        if len(df_s) < n:\n",
    "            continue\n",
    "        df_sub = df_s.tail(n) if SUBSET_ANCHOR == \"last\" else df_s.head(n)\n",
    "        suffix = f\"{SUBSET_ANCHOR}{n}d\"\n",
    "        df_sub.to_csv(out_dir / f\"{name}__{suffix}.csv\", index=False)\n",
    "        saved_sub += 1\n",
    "\n",
    "print(f\"Saved {saved} full scenario CSV files into: {out_dir.resolve()}\")\n",
    "print(f\"Saved {saved_sub} subset CSV files (sizes={SUBSET_DAYS}, anchor='{SUBSET_ANCHOR}')\")\n",
    "print(\"Example files:\", sorted([p.name for p in out_dir.glob('*.csv')])[:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9af488",
   "metadata": {},
   "source": [
    "## 5) Vizuelizacija podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def load_sales_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a CSV with at least: date, sales. Extra columns are kept if present.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'date' column\")\n",
    "    if \"sales\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'sales' column\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\").set_index(\"date\")\n",
    "    return df\n",
    "\n",
    "def plot_range(df_full: pd.DataFrame, start, end, shade_closed: bool = True, mark_spikes: bool = True):\n",
    "    \"\"\"Plot full series + zoomed range + rolling mean.\"\"\"\n",
    "    if start is None or end is None:\n",
    "        return\n",
    "    start_ts = pd.Timestamp(start)\n",
    "    end_ts = pd.Timestamp(end)\n",
    "    if start_ts > end_ts:\n",
    "        start_ts, end_ts = end_ts, start_ts\n",
    "\n",
    "    df_zoom = df_full.loc[start_ts:end_ts].copy()\n",
    "\n",
    "    # Plot A: full series with selected range highlighted\n",
    "    plt.figure(figsize=(14, 3.5))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(df_full.index.to_pydatetime(), df_full[\"sales\"].values, linewidth=1)\n",
    "    # Trend line (simple linear fit) over the full series\n",
    "    s_full = df_full[\"sales\"].dropna()\n",
    "    if len(s_full) >= 2:\n",
    "        x = np.arange(len(s_full), dtype=float)\n",
    "        m, b = np.polyfit(x, s_full.values.astype(float), 1)\n",
    "        trend = m * x + b\n",
    "        ax.plot(s_full.index.to_pydatetime(), trend, color=\"red\", linewidth=2, label=\"trend\")\n",
    "    plt.axvspan(start_ts, end_ts + pd.Timedelta(days=1), alpha=0.12)\n",
    "    plt.title(\"Full series (selected range highlighted)\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot B: zoomed range\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(df_zoom.index.to_pydatetime(), df_zoom[\"sales\"].values, linewidth=1)\n",
    "\n",
    "    # Trend line (simple linear fit) over the zoomed window\n",
    "    s_zoom = df_zoom[\"sales\"].dropna()\n",
    "    if len(s_zoom) >= 2:\n",
    "        xz = np.arange(len(s_zoom), dtype=float)\n",
    "        m2, b2 = np.polyfit(xz, s_zoom.values.astype(float), 1)\n",
    "        trend2 = m2 * xz + b2\n",
    "        ax.plot(s_zoom.index.to_pydatetime(), trend2, color=\"red\", linewidth=2, label=\"trend\")\n",
    "    # Optional: mark spikes if column exists\n",
    "    if mark_spikes and \"spike\" in df_zoom.columns:\n",
    "        spike_days = df_zoom.index[df_zoom[\"spike\"].astype(bool)]\n",
    "        if len(spike_days) > 0:\n",
    "            plt.scatter(spike_days.to_pydatetime(), df_zoom.loc[spike_days, \"sales\"], s=18, label=\"spike\", zorder=3)\n",
    "\n",
    "    # Optional: mark weekly-entry (bad input) if column exists\n",
    "    if \"weekly_entry\" in df_zoom.columns:\n",
    "        we_days = df_zoom.index[df_zoom[\"weekly_entry\"].astype(bool)]\n",
    "        if len(we_days) > 0:\n",
    "            plt.scatter(we_days.to_pydatetime(), df_zoom.loc[we_days, \"sales\"], s=28, marker=\"x\", label=\"weekly entry\", zorder=4)\n",
    "\n",
    "    # Optional: shade closed days if column exists\n",
    "    if shade_closed and \"is_closed\" in df_zoom.columns:\n",
    "        closed_days = df_zoom.index[df_zoom[\"is_closed\"].astype(bool)]\n",
    "        for d in closed_days:\n",
    "            plt.axvspan(d, d + pd.Timedelta(days=1), alpha=0.08)\n",
    "\n",
    "    plt.title(f\"Zoomed range: {start_ts.date()} → {end_ts.date()}  (rows={len(df_zoom)})\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot C: zoomed 7-day rolling mean\n",
    "    plt.figure(figsize=(14, 3.5))\n",
    "    ax = plt.gca()\n",
    "    roll = df_zoom[\"sales\"].rolling(7).mean()\n",
    "    ax.plot(roll.index.to_pydatetime(), roll.values, linewidth=1)\n",
    "    plt.title(\"Zoomed range — 7-day rolling mean (smoother view)\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"sales (7d mean)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ec040",
   "metadata": {},
   "source": [
    "## 5b) Izbor podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "    \n",
    "\n",
    "# This cell builds a small interactive UI (dropdowns + date pickers) so you can:\n",
    "# 1) choose which CSV file to load from the Data folder\n",
    "# 2) choose a time window (All / Last N / Custom)\n",
    "# 3) update a global df_current that later cells (STL / forecasting) will use\n",
    "\n",
    "data_dir = Path(\"Data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_files = sorted(data_dir.glob(\"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\n",
    "        \"No CSV files found in ./Data\\n\"\n",
    "        \"Run the scenario generation + saving cell first, \"\n",
    "        \"OR copy your CSV into ./Data\"\n",
    "    )\n",
    "\n",
    "# preferred filename\n",
    "preferred_name = \"flat__yearly.csv\"\n",
    "\n",
    "# try to find preferred file\n",
    "preferred_path = next(\n",
    "    (f for f in csv_files if f.name == preferred_name),\n",
    "    None\n",
    ")\n",
    "\n",
    "# fallback logic\n",
    "default_path = preferred_path if preferred_path else csv_files[0]\n",
    "\n",
    "print(f\"Using CSV file: {default_path.name}\")\n",
    "\n",
    "df_loaded = load_sales_csv(default_path)\n",
    "\n",
    "# Global df_current used by later cells (STL/forecast). It will be updated to the selected window.\n",
    "df_current = df_loaded.copy()\n",
    "\n",
    "\n",
    "\n",
    "file_dd = widgets.Dropdown(\n",
    "    options=[p.name for p in csv_files],\n",
    "    value=default_path.name,\n",
    "    description=\"CSV:\",\n",
    "    layout=widgets.Layout(width=\"650px\"),\n",
    ")\n",
    "\n",
    "window_dd = widgets.Dropdown(\n",
    "    options=[\"All\", \"Last 30\", \"Last 100\", \"Last 300\", \"Custom\"],\n",
    "    value=\"All\",\n",
    "    description=\"Window:\",\n",
    "    layout=widgets.Layout(width=\"250px\"),\n",
    ")\n",
    "\n",
    "anchor_dd = widgets.Dropdown(\n",
    "    options=[\"Last\", \"First\"],\n",
    "    value=\"Last\",\n",
    "    description=\"Anchor:\",\n",
    "    layout=widgets.Layout(width=\"200px\"),\n",
    ")\n",
    "\n",
    "start_picker = widgets.DatePicker(description=\"Start:\", value=df_loaded.index.min().date())\n",
    "end_picker = widgets.DatePicker(description=\"End:\", value=df_loaded.index.max().date())\n",
    "\n",
    "shade_cb = widgets.Checkbox(value=True, description=\"Shade closed days (if column exists)\")\n",
    "spikes_cb = widgets.Checkbox(value=True, description=\"Mark spikes (if column exists)\")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "def _set_picker_safely(picker, value):\n",
    "    \"\"\"Set DatePicker value without triggering a refresh loop.\"\"\"\n",
    "    if picker.value == value:\n",
    "        return\n",
    "    try:\n",
    "        picker.unobserve(refresh_plot, names=\"value\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    picker.value = value\n",
    "    try:\n",
    "        picker.observe(refresh_plot, names=\"value\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _set_dropdown_safely(dropdown, value):\n",
    "    \"\"\"Set Dropdown value without triggering nested refresh_plot calls.\"\"\"\n",
    "    if dropdown.value == value:\n",
    "        return\n",
    "    try:\n",
    "        dropdown.unobserve(refresh_plot, names=\"value\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    dropdown.value = value\n",
    "    try:\n",
    "        dropdown.observe(refresh_plot, names=\"value\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def refresh_plot(*args):\n",
    "    # If you manually change Start/End DatePicker, switch Window to Custom automatically.\n",
    "    # Otherwise (All / Last N) the code will overwrite your picked dates with the computed window.\n",
    "    if args and isinstance(args[0], dict):\n",
    "        owner = args[0].get(\"owner\", None)\n",
    "        if owner in (start_picker, end_picker) and window_dd.value != \"Custom\":\n",
    "            _set_dropdown_safely(window_dd, \"Custom\")\n",
    "\n",
    "    global df_current\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        path = data_dir / file_dd.value\n",
    "        df_now = load_sales_csv(path)\n",
    "\n",
    "        min_d = df_now.index.min()\n",
    "        max_d = df_now.index.max()\n",
    "\n",
    "        # Decide the window\n",
    "        if window_dd.value == \"All\":\n",
    "            start_ts, end_ts = min_d, max_d\n",
    "        elif window_dd.value in (\"Last 30\", \"Last 100\", \"Last 300\"):\n",
    "            n = int(window_dd.value.split()[-1])\n",
    "            if anchor_dd.value == \"Last\":\n",
    "                end_ts = max_d\n",
    "                start_ts = max(min_d, end_ts - pd.Timedelta(days=n - 1))\n",
    "            else:\n",
    "                start_ts = min_d\n",
    "                end_ts = min(max_d, start_ts + pd.Timedelta(days=n - 1))\n",
    "        else:\n",
    "            # Custom: use pickers, but clamp to available range\n",
    "            sp = pd.Timestamp(start_picker.value) if start_picker.value else min_d\n",
    "            ep = pd.Timestamp(end_picker.value) if end_picker.value else max_d\n",
    "            start_ts = max(min_d, min(sp, max_d))\n",
    "            end_ts = min(max_d, max(ep, min_d))\n",
    "\n",
    "        # Update date pickers to reflect chosen window (safe: prevents refresh loop)\n",
    "        if window_dd.value != \"Custom\":\n",
    "            _set_picker_safely(start_picker, start_ts.date())\n",
    "            _set_picker_safely(end_picker, end_ts.date())\n",
    "# IMPORTANT: df_current becomes the selected window (so STL/forecast uses 30/100/300 days if you choose that)\n",
    "        df_current = df_now.loc[start_ts:end_ts].copy()\n",
    "\n",
    "        plot_range(df_now, start_ts, end_ts, shade_closed=shade_cb.value, mark_spikes=spikes_cb.value)\n",
    "\n",
    "for w in (file_dd, window_dd, anchor_dd, start_picker, end_picker, shade_cb, spikes_cb):\n",
    "    w.observe(refresh_plot, names=\"value\")\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    file_dd,\n",
    "    widgets.HBox([window_dd, anchor_dd]),\n",
    "    widgets.HBox([start_picker, end_picker]),\n",
    "    shade_cb,\n",
    "    spikes_cb\n",
    "])\n",
    "display(controls, out)\n",
    "refresh_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b38a6",
   "metadata": {},
   "source": [
    "## 5c) Podela na trening i test (po datumu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9190d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c) Podela na trening i test (po datumu)\n",
    "# Ova sekcija koristi df_current iz 5b (izabrani fajl + prozor) i onda ti omogućava\n",
    "# da odrediš tačan date range za trening i test.\n",
    "#\n",
    "# Rezultat:\n",
    "# - df_train, df_test (global)  -> koriste se za fit/evaluaciju u forecast ćeliji\n",
    "# - df_current (global)         -> postavljamo na df_train, da STL/ACF rade nad trening skupom\n",
    "\n",
    "try:\n",
    "    df_window = df_current.copy()\n",
    "except NameError:\n",
    "    raise RuntimeError(\"df_current nije definisan. Pokreni 5b (Izbor podataka) pre 5c.\")\n",
    "\n",
    "if \"sales\" not in df_window.columns:\n",
    "    raise RuntimeError(\"Očekujem kolonu 'sales' u df_current. Proveri CSV format.\")\n",
    "\n",
    "# Uveri se da je index datetime i sortiran\n",
    "df_window = df_window.sort_index()\n",
    "df_window = df_window[~df_window.index.duplicated(keep=\"last\")]\n",
    "\n",
    "min_d = df_window.index.min()\n",
    "max_d = df_window.index.max()\n",
    "\n",
    "# Default: poslednjih 30 dana je test (ako ima dovoljno podataka)\n",
    "_default_test_days = 250\n",
    "# Za kratke serije (npr. 30/100 dana) 250 nema smisla -> koristi 20% (min 7 dana).\n",
    "# Za duže serije (>=300 dana) default je 250 (traži maksimalnu tačnost za dugi horizon).\n",
    "if len(df_window) < 300:\n",
    "    _default_test_days = max(7, int(len(df_window) * 0.2))\n",
    "test_end_default = max_d\n",
    "test_start_default = max(min_d, test_end_default - pd.Timedelta(days=_default_test_days - 1))\n",
    "train_start_default = min_d\n",
    "train_end_default = test_start_default - pd.Timedelta(days=1)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "train_start = widgets.DatePicker(description=\"Train start:\", value=train_start_default.date())\n",
    "train_end   = widgets.DatePicker(description=\"Train end:\",   value=train_end_default.date())\n",
    "test_start  = widgets.DatePicker(description=\"Test start:\",  value=test_start_default.date())\n",
    "test_end    = widgets.DatePicker(description=\"Test end:\",    value=test_end_default.date())\n",
    "\n",
    "# Brzi izbor test dužine (opciono)\n",
    "quick_test = widgets.Dropdown(\n",
    "    options=[\"Custom\", \"Last 30\", \"Last 100\", \"Last 300\"],\n",
    "    value=\"Custom\" if _default_test_days != 30 else \"Last 30\",\n",
    "    description=\"Quick test:\",\n",
    "    layout=widgets.Layout(width=\"220px\")\n",
    ")\n",
    "\n",
    "apply_btn = widgets.Button(description=\"Apply split\", button_style=\"\")\n",
    "out = widgets.Output()\n",
    "\n",
    "def _clamp_ts(ts):\n",
    "    ts = pd.Timestamp(ts)\n",
    "    if ts < min_d:\n",
    "        return min_d\n",
    "    if ts > max_d:\n",
    "        return max_d\n",
    "    return ts\n",
    "\n",
    "def _set_picker(picker, value_date):\n",
    "    # set DatePicker without extra logic/loops (we don't auto-refresh anyway)\n",
    "    picker.value = value_date\n",
    "\n",
    "def _apply_quick_test(choice):\n",
    "    # Set test range to \"last N\" and train to everything before it (no overlap)\n",
    "    if choice == \"Custom\":\n",
    "        return\n",
    "\n",
    "    n = int(choice.split()[-1])\n",
    "    end_test = max_d\n",
    "    start_test = max(min_d, end_test - pd.Timedelta(days=n - 1))\n",
    "    end_train = start_test - pd.Timedelta(days=1)\n",
    "\n",
    "    # If the dataset is too short, end_train may fall before min_d\n",
    "    if end_train < min_d:\n",
    "        end_train = min_d\n",
    "\n",
    "    _set_picker(test_end, end_test.date())\n",
    "    _set_picker(test_start, start_test.date())\n",
    "    _set_picker(train_start, min_d.date())\n",
    "    _set_picker(train_end, end_train.date())\n",
    "\n",
    "def _make_split(df_now, tr_s, tr_e, te_s, te_e):\n",
    "    tr_s = _clamp_ts(tr_s)\n",
    "    tr_e = _clamp_ts(tr_e)\n",
    "    te_s = _clamp_ts(te_s)\n",
    "    te_e = _clamp_ts(te_e)\n",
    "\n",
    "    # normalize inside each range\n",
    "    if tr_e < tr_s:\n",
    "        tr_s, tr_e = tr_e, tr_s\n",
    "    if te_e < te_s:\n",
    "        te_s, te_e = te_e, te_s\n",
    "\n",
    "    # no overlap: train end must be before test start\n",
    "    if tr_e >= te_s:\n",
    "        raise ValueError(\"Nevažeći split: TRAIN_END mora biti pre TEST_START (bez preklapanja).\")\n",
    "\n",
    "    df_tr = df_now.loc[tr_s:tr_e].copy()\n",
    "    df_te = df_now.loc[te_s:te_e].copy()\n",
    "\n",
    "    if len(df_tr) == 0:\n",
    "        raise ValueError(\"Trening opseg je prazan (0 redova).\")\n",
    "    if len(df_te) == 0:\n",
    "        raise ValueError(\"Test opseg je prazan (0 redova).\")\n",
    "\n",
    "    return df_tr, df_te\n",
    "\n",
    "def apply_split(_=None):\n",
    "    global df_train, df_test, df_current\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        # optional quick preset\n",
    "        _apply_quick_test(quick_test.value)\n",
    "\n",
    "        try:\n",
    "            df_train, df_test = _make_split(\n",
    "                df_window,\n",
    "                train_start.value, train_end.value,\n",
    "                test_start.value, test_end.value\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Split error:\", e)\n",
    "            return\n",
    "\n",
    "        # STL/ACF da rade nad trening setom\n",
    "        df_current = df_train.copy()\n",
    "\n",
    "        print(\"WINDOW:\", min_d.date(), \"->\", max_d.date(), f\"(rows={len(df_window)})\")\n",
    "        print(\"TRAIN: \", df_train.index.min().date(), \"->\", df_train.index.max().date(), f\"(rows={len(df_train)})\")\n",
    "        print(\"TEST:  \", df_test.index.min().date(),  \"->\", df_test.index.max().date(),  f\"(rows={len(df_test)})\")\n",
    "\n",
    "        # plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df_window.index.to_pydatetime(), df_window[\"sales\"].values, linewidth=1, label=\"sales\")\n",
    "\n",
    "        plt.axvspan(df_train.index.min().to_pydatetime(), df_train.index.max().to_pydatetime(), alpha=0.15, label=\"train\")\n",
    "        plt.axvspan(df_test.index.min().to_pydatetime(), df_test.index.max().to_pydatetime(), alpha=0.15, color=\"green\", label=\"test\")\n",
    "        plt.axvline(df_test.index.min().to_pydatetime(), linestyle=\"--\", linewidth=2)\n",
    "\n",
    "        plt.title(\"Podela na trening i test (po datumu)\")\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.ylabel(\"sales\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "apply_btn.on_click(apply_split)\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<b>Izaberi date range za trening i test</b>\"),\n",
    "    quick_test,\n",
    "    widgets.HBox([train_start, train_end]),\n",
    "    widgets.HBox([test_start, test_end]),\n",
    "    apply_btn\n",
    "])\n",
    "\n",
    "display(controls, out)\n",
    "apply_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b78bc",
   "metadata": {},
   "source": [
    "## 6) Diagnostics: STL decomposition + ACF (weekly/monthly/yearly signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Diagnostics: STL decomposition + ACF (weekly/monthly/yearly signals)\n",
    "# IMPORTANT: this section uses df_train\n",
    "\n",
    "try:\n",
    "    _train = df_train.copy()\n",
    "except NameError:\n",
    "    raise RuntimeError(\"df_train is not defined. Run Section 5c (train/test split) first.\")\n",
    "\n",
    "if \"sales\" not in _train.columns:\n",
    "    raise RuntimeError(\"Expected column 'sales' in df_train. Check your CSV format.\")\n",
    "\n",
    "# Ensure clean, sorted DateTimeIndex\n",
    "_train = _train.sort_index()\n",
    "_train = _train[~_train.index.duplicated(keep=\"last\")]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: strength + labeling\n",
    "# -----------------------------\n",
    "def _strength_label(v: float) -> str:\n",
    "    if v >= 0.50:\n",
    "        return \"strong\"\n",
    "    if v >= 0.20:\n",
    "        return \"moderate\"\n",
    "    if v >= 0.10:\n",
    "        return \"weak\"\n",
    "    return \"none\"\n",
    "\n",
    "def stl_strength(y: pd.Series, period: int, robust: bool = True):\n",
    "    \"\"\"Return (res, seasonal_strength, trend_strength) using Hyndman-style measures on log1p(y).\"\"\"\n",
    "    y = y.astype(float).copy().clip(lower=0)\n",
    "    y_log = np.log1p(y)\n",
    "\n",
    "    res = STL(y_log, period=period, robust=robust).fit()\n",
    "    seasonal, trend, resid = res.seasonal, res.trend, res.resid\n",
    "\n",
    "    def _var(a):\n",
    "        return float(np.var(a, ddof=0))\n",
    "\n",
    "    denom_s = _var(resid + seasonal)\n",
    "    denom_t = _var(resid + trend)\n",
    "\n",
    "    seasonal_strength = max(0.0, 1.0 - (_var(resid) / denom_s)) if denom_s > 0 else 0.0\n",
    "    trend_strength = max(0.0, 1.0 - (_var(resid) / denom_t)) if denom_t > 0 else 0.0\n",
    "\n",
    "    return res, seasonal_strength, trend_strength\n",
    "\n",
    "# -----------------------------\n",
    "# A) Seasonality detection (STL per candidate period)\n",
    "# -----------------------------\n",
    "y = _train[\"sales\"].dropna()\n",
    "N = len(y)\n",
    "\n",
    "candidate_periods = [7, 30, 365]  # daily data candidates\n",
    "valid_periods = [p for p in candidate_periods if p >= 2 and N >= 2 * p]  # need ~2 cycles\n",
    "\n",
    "print(f\"Train rows: {N} | valid STL periods: {valid_periods if valid_periods else 'none'}\")\n",
    "\n",
    "SEASONAL_THR = 0.20  # \"moderate or stronger\"\n",
    "\n",
    "period_results = []\n",
    "for p in valid_periods:\n",
    "    res_p, ss, ts = stl_strength(y, period=p, robust=True)\n",
    "    period_results.append({\"period\": p, \"seasonal_strength\": ss, \"trend_strength\": ts, \"res\": res_p})\n",
    "\n",
    "if period_results:\n",
    "    print(\"\\nSeasonality/Trend strength per period (STL on log1p(sales)):\")\n",
    "    for r in sorted(period_results, key=lambda d: d[\"period\"]):\n",
    "        p = r[\"period\"]\n",
    "        ss = r[\"seasonal_strength\"]\n",
    "        ts = r[\"trend_strength\"]\n",
    "        print(f\"  period={p:>3} | seasonal={ss:.3f} ({_strength_label(ss)}) | trend={ts:.3f} ({_strength_label(ts)})\")\n",
    "\n",
    "    seasonality_by_period = {r[\"period\"]: (r[\"seasonal_strength\"] >= SEASONAL_THR) for r in period_results}\n",
    "\n",
    "    print(\"\\nDetected seasonality (threshold >= 0.20):\")\n",
    "    for p in sorted(seasonality_by_period):\n",
    "        print(f\"  period={p:>3}: {'YES' if seasonality_by_period[p] else 'NO'}\")\n",
    "\n",
    "    seasonality_exists = any(seasonality_by_period.values())\n",
    "\n",
    "    # choose which STL to plot (prefer yearly if it's at least weak)\n",
    "    by_period = {r[\"period\"]: r for r in period_results}\n",
    "    if 365 in by_period and by_period[365][\"seasonal_strength\"] >= 0.10:\n",
    "        chosen_period = 365\n",
    "    else:\n",
    "        chosen_period = max(period_results, key=lambda d: d[\"seasonal_strength\"])[\"period\"]\n",
    "    chosen_res = by_period[chosen_period][\"res\"]\n",
    "\n",
    "else:\n",
    "    print(\"Seasonality: NO (not enough data for STL on 7/30/365)\")\n",
    "    seasonality_exists = False\n",
    "    chosen_res = None\n",
    "    chosen_period = None\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# A2) Model seasonality selection for downstream models\n",
    "# Priority: yearly > monthly > weekly\n",
    "# - GOOD_SEASONALITIES: all that passed SEASONAL_THR\n",
    "# - SELECTED_SEASONALITY / SELECTED_SEASONAL_PERIOD: single best (for ETS, etc.)\n",
    "# -----------------------------\n",
    "period_to_name = {7: \"weekly\", 30: \"monthly\", 365: \"yearly\"}\n",
    "name_to_period = {\"weekly\": 7, \"monthly\": 30, \"yearly\": 365}\n",
    "_priority = [\"yearly\", \"monthly\", \"weekly\"]\n",
    "\n",
    "if \"period_results\" in locals() and period_results:\n",
    "    # seasonality_by_period is defined in the block above\n",
    "    _sbp = dict(seasonality_by_period)\n",
    "    SEASONALITY_SCORES = {\n",
    "        period_to_name.get(r[\"period\"], str(r[\"period\"])): float(r[\"seasonal_strength\"])\n",
    "        for r in period_results\n",
    "        if r[\"period\"] in period_to_name\n",
    "    }\n",
    "else:\n",
    "    _sbp = {}\n",
    "    SEASONALITY_SCORES = {}\n",
    "\n",
    "GOOD_SEASONALITIES = [period_to_name[p] for p, ok in _sbp.items() if ok and p in period_to_name]\n",
    "GOOD_SEASONALITIES = [s for s in _priority if s in GOOD_SEASONALITIES]\n",
    "\n",
    "SELECTED_SEASONALITY = next((s for s in _priority if s in GOOD_SEASONALITIES), None)\n",
    "SELECTED_SEASONAL_PERIOD = name_to_period.get(SELECTED_SEASONALITY)\n",
    "\n",
    "print(\"\\nSelected seasonality for modeling (priority yearly > monthly > weekly):\",\n",
    "      SELECTED_SEASONALITY, \"| period:\", SELECTED_SEASONAL_PERIOD)\n",
    "print(\"Good seasonalities:\", GOOD_SEASONALITIES)\n",
    "\n",
    "# Export for other cells\n",
    "globals()[\"GOOD_SEASONALITIES\"] = GOOD_SEASONALITIES\n",
    "globals()[\"SELECTED_SEASONALITY\"] = SELECTED_SEASONALITY\n",
    "globals()[\"SELECTED_SEASONAL_PERIOD\"] = SELECTED_SEASONAL_PERIOD\n",
    "globals()[\"SEASONALITY_SCORES\"] = SEASONALITY_SCORES\n",
    "\n",
    "# -----------------------------\n",
    "# B) Trend detection on the WHOLE training dataset (no STL decision logic)\n",
    "# -----------------------------\n",
    "# Idea: fit a line to log1p(sales) across time and test if slope is meaningful.\n",
    "# This uses the entire training set (all points), not a decomposed component.\n",
    "\n",
    "from scipy.stats import linregress, theilslopes\n",
    "\n",
    "y_raw = _train[\"sales\"].astype(float).clip(lower=0)\n",
    "y_log = np.log1p(y_raw)\n",
    "\n",
    "# Use \"days since start\" as x\n",
    "x_days = (_train.index - _train.index.min()).days.astype(float)\n",
    "\n",
    "# Drop NaNs consistently\n",
    "mask = np.isfinite(x_days) & np.isfinite(y_log.values)\n",
    "x = x_days[mask]\n",
    "yl = y_log.values[mask]\n",
    "\n",
    "if len(yl) < 10:\n",
    "    print(\"\\nTrend (whole train): NOT ENOUGH DATA (need ~10+ points)\")\n",
    "    trend_exists = False\n",
    "else:\n",
    "    lr = linregress(x, yl)  # slope per day (log1p units), p-value included\n",
    "    slope_day = float(lr.slope)\n",
    "    r2 = float(lr.rvalue ** 2)\n",
    "    pval = float(lr.pvalue)\n",
    "\n",
    "    # Robust slope (less sensitive to spikes)\n",
    "    ts = theilslopes(yl, x, 0.95)  # slope, intercept, lo_slope, hi_slope\n",
    "    slope_day_robust = float(ts[0])\n",
    "    lo, hi = float(ts[2]), float(ts[3])\n",
    "\n",
    "    # Convert slope -> approx multiplicative change in (1+sales)\n",
    "    yearly_factor = float(np.exp(slope_day * 365.0))\n",
    "    total_days = float(np.max(x) - np.min(x)) if len(x) else 0.0\n",
    "    total_factor = float(np.exp(slope_day * total_days)) if total_days > 0 else 1.0\n",
    "\n",
    "    # Decision (heuristic): statistically non-zero + practically meaningful\n",
    "    # \"practically meaningful\": >= ~5% per year OR >= ~10% over entire training span\n",
    "    PRACTICAL_YEARLY = 1.05\n",
    "    PRACTICAL_TOTAL = 1.10\n",
    "\n",
    "    significant = (pval < 0.05) and (lo > 0 or hi < 0)  # CI excludes 0\n",
    "    practical = (yearly_factor >= PRACTICAL_YEARLY) or (yearly_factor <= 1.0 / PRACTICAL_YEARLY) or                 (total_factor >= PRACTICAL_TOTAL) or (total_factor <= 1.0 / PRACTICAL_TOTAL)\n",
    "\n",
    "    trend_exists = bool(significant and practical)\n",
    "\n",
    "    direction = \"rising\" if slope_day > 0 else (\"falling\" if slope_day < 0 else \"flat\")\n",
    "\n",
    "    print(\"\\nTrend (whole train, log1p regression):\")\n",
    "    print(f\"  decision: {'YES' if trend_exists else 'NO'}\")\n",
    "    print(f\"  direction: {direction}\")\n",
    "    print(f\"  slope/day: {slope_day:.6f} | robust slope/day: {slope_day_robust:.6f} (95% CI [{lo:.6f}, {hi:.6f}])\")\n",
    "    print(f\"  R²: {r2:.3f} | p-value: {pval:.3g}\")\n",
    "    print(f\"  approx yearly factor (1+sales): {yearly_factor:.3f} | approx total factor: {total_factor:.3f} over {int(total_days)} days\")\n",
    "\n",
    "print(f\"\\nSummary: Seasonality={'YES' if seasonality_exists else 'NO'} | Trend={'YES' if trend_exists else 'NO'}\")\n",
    "\n",
    "# -----------------------------\n",
    "# C) Plot STL decomposition (chosen)\n",
    "# -----------------------------\n",
    "if chosen_res is not None:\n",
    "    chosen_res.plot()\n",
    "    plt.suptitle(f\"STL decomposition (train) — period={chosen_period}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"STL plot skipped: not enough data for selected periods. (Need at least ~2 cycles.)\")\n",
    "\n",
    "# -----------------------------\n",
    "# D) Plot ACF (train)\n",
    "# -----------------------------\n",
    "x_acf = y.dropna()\n",
    "max_lags = min(400, len(x_acf) - 1)\n",
    "\n",
    "if max_lags < 1:\n",
    "    print(\"ACF skipped: not enough data points after dropping NaNs.\")\n",
    "else:\n",
    "    plot_acf(x_acf, lags=max_lags)\n",
    "    plt.title(f\"Autocorrelation (ACF) — train, lags={max_lags} (spikes at 7/~30/~365 if present)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74862d7",
   "metadata": {},
   "source": [
    "## 7) Optional: simulate missing dates + show two filling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs = df_current.copy()\n",
    "\n",
    "missing_frac = 0.02\n",
    "mask_missing = np.random.rand(len(df_obs)) < missing_frac\n",
    "df_missing = df_obs.loc[~mask_missing].copy()\n",
    "\n",
    "full_idx = pd.date_range(df_obs.index.min(), df_obs.index.max(), freq=\"D\")\n",
    "df_missing = df_missing.reindex(full_idx)\n",
    "\n",
    "print(\"Missing days simulated:\", int(df_missing[\"sales\"].isna().sum()))\n",
    "\n",
    "# Strategy A: fill missing with 0\n",
    "df_fill_zero = df_missing.copy()\n",
    "df_fill_zero[\"sales\"] = df_fill_zero[\"sales\"].fillna(0)\n",
    "\n",
    "# Strategy B: fill missing with weekday median\n",
    "df_fill_wkmed = df_missing.copy()\n",
    "weekday_medians = df_obs.groupby(df_obs.index.dayofweek)[\"sales\"].median()\n",
    "wk = pd.Series(df_fill_wkmed.index.dayofweek, index=df_fill_wkmed.index)\n",
    "df_fill_wkmed[\"sales\"] = df_fill_wkmed[\"sales\"].fillna(wk.map(weekday_medians))\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "df_obs[\"sales\"].plot(label=\"original\")\n",
    "df_fill_zero[\"sales\"].plot(alpha=0.7, label=\"filled with 0\")\n",
    "df_fill_wkmed[\"sales\"].plot(alpha=0.7, label=\"filled with weekday median\")\n",
    "plt.title(\"Missing-date filling strategies (compare)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5f122",
   "metadata": {},
   "source": [
    "## 8) Example forecasts (ETS + Prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8a) ETS (Holt-Winters / Exponential Smoothing) — seasonality from Step 6 (single best)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "try:\n",
    "    _df = df_current\n",
    "except NameError:\n",
    "    raise RuntimeError(\"df_current is not defined. Run Section 5 (load CSV + choose range) first.\")\n",
    "\n",
    "# Prefer train/test split if you ran section 5c\n",
    "if \"df_train\" in globals() and \"df_test\" in globals() and len(df_train) > 0 and len(df_test) > 0:\n",
    "    _train = df_train.copy()\n",
    "    _test = df_test.copy()\n",
    "    HORIZON_DAYS = len(_test)\n",
    "    print(\"Koristim df_train za fit, df_test za evaluaciju.\")\n",
    "else:\n",
    "    _train = _df.copy()\n",
    "    _test = None\n",
    "    HORIZON_DAYS = 180\n",
    "    print(\"Koristim df_current (nema df_train/df_test).\")\n",
    "\n",
    "# Ensure DateTimeIndex daily (fill missing days with 0 for stability)\n",
    "_train = _train.sort_index()\n",
    "_train = _train.asfreq(\"D\")\n",
    "_train[\"sales\"] = _train[\"sales\"].astype(float).fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "if _test is not None:\n",
    "    _test = _test.sort_index()\n",
    "    _test = _test.asfreq(\"D\")\n",
    "    _test[\"sales\"] = _test[\"sales\"].astype(float).fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "# Seasonality from Step 6 (single best; priority yearly > monthly > weekly)\n",
    "SEL_PERIOD = globals().get(\"SELECTED_SEASONAL_PERIOD\", None)  # None / 7 / 30 / 365\n",
    "if SEL_PERIOD is None:\n",
    "    seasonal = None\n",
    "    seasonal_periods = None\n",
    "else:\n",
    "    seasonal = \"add\"\n",
    "    seasonal_periods = int(SEL_PERIOD)\n",
    "\n",
    "print(\"ETS seasonal_periods:\", seasonal_periods)\n",
    "\n",
    "ets = ExponentialSmoothing(\n",
    "    _train[\"sales\"],\n",
    "    trend=\"add\",\n",
    "    seasonal=seasonal,\n",
    "    seasonal_periods=seasonal_periods,\n",
    ").fit(optimized=True)\n",
    "\n",
    "ets_fc = ets.forecast(HORIZON_DAYS)\n",
    "\n",
    "# Align forecast index\n",
    "if _test is not None:\n",
    "    ets_fc = pd.Series(ets_fc.values, index=_test.index, name=\"yhat\").clip(lower=0.0)\n",
    "else:\n",
    "    future_idx = pd.date_range(_train.index.max() + pd.Timedelta(days=1), periods=HORIZON_DAYS, freq=\"D\")\n",
    "    ets_fc = pd.Series(ets_fc.values, index=future_idx, name=\"yhat\").clip(lower=0.0)\n",
    "\n",
    "# Metrics (if test exists)\n",
    "def _metrics(y_true: pd.Series, y_pred: pd.Series):\n",
    "    y_true = y_true.astype(float).values\n",
    "    y_pred = y_pred.astype(float).values\n",
    "    mae = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "    denom = np.where(y_true == 0, 1.0, y_true)\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n",
    "\n",
    "if _test is not None:\n",
    "    print(\"ETS metrics:\", _metrics(_test[\"sales\"].loc[ets_fc.index], ets_fc))\n",
    "\n",
    "# Plot\n",
    "VIEW_LAST_TRAIN_DAYS = 200\n",
    "train_view_start = max(_train.index.min(), _train.index.max() - pd.Timedelta(days=VIEW_LAST_TRAIN_DAYS - 1))\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(_train.loc[train_view_start:].index, _train.loc[train_view_start:, \"sales\"].values, label=\"train\", linewidth=2)\n",
    "\n",
    "if _test is not None:\n",
    "    plt.plot(_test.index, _test[\"sales\"].values, label=\"test\", linewidth=2, color=\"green\")\n",
    "    plt.axvline(_test.index.min(), linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.plot(ets_fc.index, ets_fc.values, label=\"ETS forecast\", linewidth=2)\n",
    "plt.title(\"ETS — train/test/forecast\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"sales\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d178b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8b) Prophet — enable ALL \"good\" seasonalities from Step 6 (can be multiple)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Prophet is not available. Install with: pip install prophet\") from e\n",
    "\n",
    "try:\n",
    "    _df = df_current\n",
    "except NameError:\n",
    "    raise RuntimeError(\"df_current is not defined. Run Section 5 (load CSV + choose range) first.\")\n",
    "\n",
    "# Prefer train/test split if you ran section 5c\n",
    "if \"df_train\" in globals() and \"df_test\" in globals() and len(df_train) > 0 and len(df_test) > 0:\n",
    "    _train = df_train.copy()\n",
    "    _test = df_test.copy()\n",
    "    HORIZON_DAYS = len(_test)\n",
    "    print(\"Koristim df_train za fit, df_test za evaluaciju.\")\n",
    "else:\n",
    "    _train = _df.copy()\n",
    "    _test = None\n",
    "    HORIZON_DAYS = 180\n",
    "    print(\"Koristim df_current (nema df_train/df_test).\")\n",
    "\n",
    "_train = _train.sort_index().asfreq(\"D\")\n",
    "_train[\"sales\"] = _train[\"sales\"].astype(float).fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "if _test is not None:\n",
    "    _test = _test.sort_index().asfreq(\"D\")\n",
    "    _test[\"sales\"] = _test[\"sales\"].astype(float).fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "GOOD = globals().get(\"GOOD_SEASONALITIES\", [])\n",
    "print(\"Good seasonalities from Step 6:\", GOOD)\n",
    "\n",
    "if GOOD:\n",
    "    m = Prophet(\n",
    "        yearly_seasonality=(\"yearly\" in GOOD),\n",
    "        weekly_seasonality=(\"weekly\" in GOOD),\n",
    "        daily_seasonality=False,\n",
    "    )\n",
    "else:\n",
    "    # If Step 6 didn't confirm anything, keep Prophet's defaults (auto) instead of forcing none.\n",
    "    m = Prophet(\n",
    "        yearly_seasonality=\"auto\",\n",
    "        weekly_seasonality=\"auto\",\n",
    "        daily_seasonality=False,\n",
    "    )\n",
    "\n",
    "# Monthly is not built-in -> add custom monthly if detected as good\n",
    "if \"monthly\" in GOOD:\n",
    "    m.add_seasonality(name=\"monthly\", period=30.5, fourier_order=5)\n",
    "\n",
    "# Build Prophet input with required columns: ds (datetime), y (numeric)\n",
    "def _to_prophet_df(obj, y_col: str = \"sales\") -> pd.DataFrame:\n",
    "    if isinstance(obj, pd.Series):\n",
    "        obj = obj.to_frame(name=y_col)\n",
    "\n",
    "    df = obj.copy()\n",
    "\n",
    "    # Find target column\n",
    "    if y_col not in df.columns:\n",
    "        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if not numeric_cols:\n",
    "            raise ValueError(f\"Prophet: can't find numeric target column. Columns: {list(df.columns)}\")\n",
    "        y_col = numeric_cols[0]\n",
    "\n",
    "    # Case 1: datetime index (preferred)\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        out = df[[y_col]].reset_index()\n",
    "        out.columns = [\"ds\", \"y\"]\n",
    "    else:\n",
    "        # Case 2: look for an existing date column\n",
    "        date_col = None\n",
    "        for c in [\"ds\", \"date\", \"Date\", \"timestamp\", \"time\"]:\n",
    "            if c in df.columns:\n",
    "                date_col = c\n",
    "                break\n",
    "\n",
    "        if date_col is None:\n",
    "            tmp = df.reset_index()\n",
    "            date_col = tmp.columns[0]\n",
    "            out = tmp[[date_col, y_col]].rename(columns={date_col: \"ds\", y_col: \"y\"})\n",
    "        else:\n",
    "            out = df[[date_col, y_col]].rename(columns={date_col: \"ds\", y_col: \"y\"})\n",
    "\n",
    "    out[\"ds\"] = pd.to_datetime(out[\"ds\"], errors=\"coerce\")\n",
    "    # Ensure tz-naive datetimes for Prophet\n",
    "    try:\n",
    "        out[\"ds\"] = out[\"ds\"].dt.tz_localize(None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out[\"y\"] = pd.to_numeric(out[\"y\"], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[\"ds\", \"y\"]).sort_values(\"ds\")\n",
    "\n",
    "    if out.shape[0] < 2:\n",
    "        raise ValueError(\"Prophet: not enough valid rows after ds/y conversion.\")\n",
    "    return out\n",
    "\n",
    "df_prophet = _to_prophet_df(_train, y_col=\"sales\")\n",
    "m.fit(df_prophet)\n",
    "\n",
    "future = m.make_future_dataframe(periods=HORIZON_DAYS, freq=\"D\")\n",
    "fcst = m.predict(future).set_index(\"ds\")\n",
    "\n",
    "# Build forecast series aligned to test (if exists)\n",
    "if _test is not None:\n",
    "    yhat = fcst.loc[_test.index, \"yhat\"].clip(lower=0.0)\n",
    "    yhat = pd.Series(yhat.values, index=_test.index, name=\"yhat\")\n",
    "else:\n",
    "    yhat = fcst[\"yhat\"].iloc[-HORIZON_DAYS:].clip(lower=0.0)\n",
    "    yhat = pd.Series(yhat.values, index=yhat.index, name=\"yhat\")\n",
    "\n",
    "def _metrics(y_true: pd.Series, y_pred: pd.Series):\n",
    "    y_true = y_true.astype(float).values\n",
    "    y_pred = y_pred.astype(float).values\n",
    "    mae = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "    denom = np.where(y_true == 0, 1.0, y_true)\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n",
    "\n",
    "if _test is not None:\n",
    "    print(\"Prophet metrics:\", _metrics(_test[\"sales\"].loc[yhat.index], yhat))\n",
    "\n",
    "# Plot: train vs test vs forecast (yhat)\n",
    "VIEW_LAST_TRAIN_DAYS = 200\n",
    "train_view_start = max(_train.index.min(), _train.index.max() - pd.Timedelta(days=VIEW_LAST_TRAIN_DAYS - 1))\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(_train.loc[train_view_start:].index, _train.loc[train_view_start:, \"sales\"].values, label=\"train\", linewidth=2)\n",
    "\n",
    "if _test is not None:\n",
    "    plt.plot(_test.index, _test[\"sales\"].values, label=\"test\", linewidth=2, color=\"green\")\n",
    "    plt.axvline(_test.index.min(), linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.plot(yhat.index, yhat.values, label=\"Prophet forecast\", linewidth=2)\n",
    "plt.title(\"Prophet — train/test/forecast\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"sales\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Components plot (trend + seasonality)\n",
    "_ = m.plot_components(fcst.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84a37f",
   "metadata": {},
   "source": [
    "### 8c) ARIMA, Linear Regression i XGBoost (lag feature modeli)\n",
    "\n",
    "U 8c je svaka metoda podeljena na **logiku** i **grafik**:\n",
    "- **Linear Regression**: 8c-1a (logika) + 8c-1b (grafik)\n",
    "- **XGBoost**: 8c-2a (logika) + 8c-2b (grafik)\n",
    "- **ARIMA/SARIMAX**: 8c-3a (logika) + 8c-3b (grafik)\n",
    "\n",
    "Na kraju (8d) dobijaš **Plotly** grafikon gde možeš:\n",
    "- da zumiraš izborom datuma (range slider / drag-select)\n",
    "- da uključuješ/isključuješ modele (toggle dugmad + legenda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8c-0) Zajednička priprema (df_train/df_test + feature funkcije + metrics)\n",
    "# Očekuje da si već pokrenuo 5c (train/test split), pa imaš df_train i df_test.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Učitaj train/test (strogo)\n",
    "# -----------------------------\n",
    "if \"df_train\" not in globals() or \"df_test\" not in globals():\n",
    "    raise RuntimeError(\"df_train / df_test nisu definisani. Pokreni 5c (podela na trening/test) pre 8c.\")\n",
    "\n",
    "_train = df_train.copy()\n",
    "_test  = df_test.copy()\n",
    "\n",
    "if \"sales\" not in _train.columns or \"sales\" not in _test.columns:\n",
    "    raise RuntimeError(\"Očekujem kolonu 'sales' u df_train i df_test. Proveri CSV format.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Helper: obezbedi dnevni indeks bez rupa (missing days -> sales=0)\n",
    "# -----------------------------\n",
    "def ensure_daily(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy().sort_index()\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    start, end = df.index.min(), df.index.max()\n",
    "    full = pd.date_range(start, end, freq=\"D\")\n",
    "\n",
    "    df = df.reindex(full)\n",
    "    df.index.name = \"date\"\n",
    "\n",
    "    # Sales: missing -> 0\n",
    "    df[\"sales\"] = pd.to_numeric(df[\"sales\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Ostale kolone (ako postoje): stabilno popunjavanje bez FutureWarning/Downcasting warning-a\n",
    "    for col in df.columns:\n",
    "        if col == \"sales\":\n",
    "            continue\n",
    "\n",
    "        s = df[col]\n",
    "\n",
    "        # \"boolean-ish\": dozvoljene vrednosti {0,1,True,False} (+ NaN)\n",
    "        try:\n",
    "            is_boolish = s.dropna().isin([0, 1, True, False]).all()\n",
    "        except Exception:\n",
    "            is_boolish = False\n",
    "\n",
    "        if is_boolish:\n",
    "            # Pretvori object 0/1 u numeric pa u bool, pa popuni NaN -> False\n",
    "            if s.dtype == object:\n",
    "                s_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "                if s_num.notna().any():\n",
    "                    s = s_num\n",
    "            s = s.fillna(0).astype(int).astype(bool)\n",
    "            df[col] = s\n",
    "        else:\n",
    "            # Za \"numeric-ish\" ili druge kolone: ffill pa 0\n",
    "            if s.dtype == object:\n",
    "                s = s.infer_objects(copy=False)\n",
    "            df[col] = s.ffill().fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "_train_d = ensure_daily(_train)\n",
    "_test_d  = ensure_daily(_test)\n",
    "\n",
    "train_series = _train_d[\"sales\"].astype(float)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Feature engineering (lag modeli)\n",
    "# -----------------------------\n",
    "LAGS  = [1, 7, 14, 30]\n",
    "ROLLS = [7, 14]\n",
    "\n",
    "def make_features_from_history(history: pd.Series, ts: pd.Timestamp) -> dict:\n",
    "    \"\"\"Feature vektor za datum ts koristeći SAMO istoriju do ts-1.\"\"\"\n",
    "    feats = {}\n",
    "    feats[\"dow\"] = int(ts.dayofweek)\n",
    "    feats[\"month\"] = int(ts.month)\n",
    "    feats[\"dayofyear\"] = int(ts.dayofyear)\n",
    "\n",
    "    for l in LAGS:\n",
    "        feats[f\"lag_{l}\"] = float(history.iloc[-l]) if len(history) >= l else 0.0\n",
    "\n",
    "    for k in ROLLS:\n",
    "        feats[f\"roll_mean_{k}\"] = float(history.iloc[-k:].mean()) if len(history) >= k else 0.0\n",
    "\n",
    "    return feats\n",
    "\n",
    "def build_supervised(df_daily: pd.DataFrame):\n",
    "    \"\"\"X, y iz dnevnog df: y(t)=sales(t), feature-i koriste prošlost.\"\"\"\n",
    "    y = df_daily[\"sales\"].astype(float)\n",
    "\n",
    "    max_need = max(LAGS + ROLLS)\n",
    "    X_rows, y_rows, idx_rows = [], [], []\n",
    "\n",
    "    for i in range(max_need, len(df_daily)):\n",
    "        ts = df_daily.index[i]\n",
    "        hist = y.iloc[:i]  # do ts-1\n",
    "        feats = make_features_from_history(hist, ts)\n",
    "        X_rows.append(feats)\n",
    "        y_rows.append(float(y.iloc[i]))\n",
    "        idx_rows.append(ts)\n",
    "\n",
    "    X = pd.DataFrame(X_rows, index=pd.DatetimeIndex(idx_rows))\n",
    "    y_out = pd.Series(y_rows, index=pd.DatetimeIndex(idx_rows), name=\"y\")\n",
    "    return X, y_out\n",
    "\n",
    "def recursive_forecast(model, history: pd.Series, start_date: pd.Timestamp, periods: int, feature_columns=None) -> pd.Series:\n",
    "    \"\"\"Rekurzivna prognoza: svaki dan koristi prethodno predviđene vrednosti kao 'istoriju'.\"\"\"\n",
    "    hist = history.copy()\n",
    "    preds, dates = [], []\n",
    "    cur = pd.Timestamp(start_date)\n",
    "\n",
    "    for _ in range(periods):\n",
    "        feats = make_features_from_history(hist, cur)\n",
    "        if feature_columns is not None:\n",
    "            X1 = pd.DataFrame([feats], columns=feature_columns)\n",
    "        else:\n",
    "            X1 = pd.DataFrame([feats])\n",
    "        yhat = float(model.predict(X1)[0])\n",
    "        yhat = max(0.0, yhat)  # clamp\n",
    "        preds.append(yhat)\n",
    "        dates.append(cur)\n",
    "        hist = pd.concat([hist, pd.Series([yhat], index=[cur])])\n",
    "        cur = cur + pd.Timedelta(days=1)\n",
    "\n",
    "    return pd.Series(preds, index=pd.DatetimeIndex(dates), name=\"yhat\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Metrics\n",
    "# -----------------------------\n",
    "def metrics(y_true: pd.Series, y_pred: pd.Series) -> dict:\n",
    "    y_true = y_true.astype(float).values\n",
    "    y_pred = y_pred.astype(float).values\n",
    "    mae = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "    denom = np.where(y_true == 0, 1.0, y_true)\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape}\n",
    "\n",
    "# Global containers (koristi 8c-1a/2a/3a i 8d)\n",
    "results = {}\n",
    "model_metrics = {}\n",
    "\n",
    "# Forecast horizon = test length\n",
    "forecast_start = _test_d.index.min()\n",
    "horizon = len(_test_d)\n",
    "\n",
    "print(f\"Train rows: {len(_train_d)} | Test rows: {len(_test_d)} | forecast_start={forecast_start.date()} | horizon={horizon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8c-1a) Linear Regression — logika (fit + prognoza + metrike)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, y_train = build_supervised(_train_d)\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, y_train)\n",
    "\n",
    "pred_lin = recursive_forecast(lin_model, train_series, forecast_start, horizon, feature_columns=list(X_train.columns))\n",
    "results[\"Linear\"] = pred_lin\n",
    "\n",
    "# metrike na test opsegu\n",
    "y_true = _test_d[\"sales\"].astype(float)\n",
    "model_metrics[\"Linear\"] = metrics(y_true.loc[pred_lin.index], pred_lin)\n",
    "\n",
    "print(\"Linear metrics:\", model_metrics[\"Linear\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8c-1b) Linear Regression — grafik (train/test/pred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "VIEW_LAST_TRAIN_DAYS = 200  # promeni po želji\n",
    "\n",
    "train_view_start = max(_train_d.index.min(), _train_d.index.max() - pd.Timedelta(days=VIEW_LAST_TRAIN_DAYS-1))\n",
    "tr = _train_d.loc[train_view_start:]\n",
    "te = _test_d.copy()\n",
    "pred = results[\"Linear\"]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(tr.index, tr[\"sales\"].values, label=\"train (actual)\", linewidth=2)\n",
    "plt.plot(te.index, te[\"sales\"].values, label=\"test (actual)\", linewidth=2, color=\"green\")\n",
    "plt.plot(pred.index, pred.values, label=\"pred (Linear)\", linewidth=2)\n",
    "plt.axvline(te.index.min(), linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Linear Regression — train/test/pred\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ced40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8c-2a) XGBoost — logika (fit + prognoza + metrike)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "X_train, y_train = build_supervised(_train_d)\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=4,\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "pred_xgb = recursive_forecast(xgb_model, train_series, forecast_start, horizon, feature_columns=list(X_train.columns))\n",
    "results[\"XGBoost\"] = pred_xgb\n",
    "\n",
    "y_true = _test_d[\"sales\"].astype(float)\n",
    "model_metrics[\"XGBoost\"] = metrics(y_true.loc[pred_xgb.index], pred_xgb)\n",
    "\n",
    "print(\"XGBoost metrics:\", model_metrics[\"XGBoost\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2dd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8c-2b) XGBoost — grafik (train/test/pred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "VIEW_LAST_TRAIN_DAYS = 200\n",
    "\n",
    "train_view_start = max(_train_d.index.min(), _train_d.index.max() - pd.Timedelta(days=VIEW_LAST_TRAIN_DAYS-1))\n",
    "tr = _train_d.loc[train_view_start:]\n",
    "te = _test_d.copy()\n",
    "pred = results[\"XGBoost\"]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(tr.index, tr[\"sales\"].values, label=\"train (actual)\", linewidth=2)\n",
    "plt.plot(te.index, te[\"sales\"].values, label=\"test (actual)\", linewidth=2, color=\"green\")\n",
    "plt.plot(pred.index, pred.values, label=\"pred (XGBoost)\", linewidth=2)\n",
    "plt.axvline(te.index.min(), linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"XGBoost — train/test/pred\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75c97a",
   "metadata": {},
   "source": [
    "## 9) Uporedjivanje svih modela (tabela + Plotly graf)\n",
    "\n",
    "Ovaj deo skuplja prognoze iz sekcije 8 (ETS, Prophet) i 8c (Linear, XGBoost, + bilo koji dodatni model u `results`) i pravi:\n",
    "- **tabelu metrika** (MAE, RMSE, MAPE, sMAPE)\n",
    "- **Plotly graf** gde su svi modeli uporedjeni na test periodu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9) Model comparison — unified table + Plotly overlay\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Preconditions (need test set)\n",
    "# -----------------------------\n",
    "if \"df_train\" not in globals() or \"df_test\" not in globals():\n",
    "    raise RuntimeError(\"Za Section 9 je potreban train/test split. Pokreni 5c (podela na trening/test) pre Section 9.\")\n",
    "\n",
    "_train_all = df_train.copy()\n",
    "_test_all  = df_test.copy()\n",
    "\n",
    "# Ensure daily DateTimeIndex + numeric sales\n",
    "_train_all = _train_all.sort_index().asfreq(\"D\")\n",
    "_test_all  = _test_all.sort_index().asfreq(\"D\")\n",
    "\n",
    "_train_all[\"sales\"] = pd.to_numeric(_train_all[\"sales\"], errors=\"coerce\").fillna(0.0).clip(lower=0.0)\n",
    "_test_all[\"sales\"]  = pd.to_numeric(_test_all[\"sales\"], errors=\"coerce\").fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "y_true = _test_all[\"sales\"].astype(float)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Helpers\n",
    "# -----------------------------\n",
    "def _to_series(x) -> pd.Series:\n",
    "    if isinstance(x, pd.Series):\n",
    "        s = x.copy()\n",
    "    elif isinstance(x, pd.DataFrame):\n",
    "        # try common column names\n",
    "        for col in [\"yhat\", \"forecast\", \"pred\", \"prediction\", \"sales\", \"y\"]:\n",
    "            if col in x.columns:\n",
    "                s = x[col].copy()\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame predikcija nema prepoznatljiv kolonu (yhat/forecast/pred/...).\")\n",
    "    else:\n",
    "        s = pd.Series(x)\n",
    "\n",
    "    if not isinstance(s.index, pd.DatetimeIndex):\n",
    "        s.index = pd.to_datetime(s.index)\n",
    "\n",
    "    s = s.sort_index()\n",
    "    s.name = \"yhat\"\n",
    "    return s\n",
    "\n",
    "def _align_to_test(pred: pd.Series, test_index: pd.DatetimeIndex) -> pd.Series:\n",
    "    s = _to_series(pred)\n",
    "\n",
    "    # Clamp negatives to 0 (sales cannot be negative)\n",
    "    s = s.astype(float).clip(lower=0.0)\n",
    "\n",
    "    # Reindex to the full test index (missing dates -> NaN)\n",
    "    return s.reindex(test_index)\n",
    "\n",
    "def _metrics(y_true: pd.Series, y_pred: pd.Series) -> dict:\n",
    "    mask = y_true.notna() & y_pred.notna()\n",
    "    yt = y_true.loc[mask].astype(float).values\n",
    "    yp = y_pred.loc[mask].astype(float).values\n",
    "\n",
    "    if len(yt) == 0:\n",
    "        return {\"N\": 0, \"MAE\": np.nan, \"RMSE\": np.nan, \"MAPE%\": np.nan, \"sMAPE%\": np.nan}\n",
    "\n",
    "    mae = float(np.mean(np.abs(yt - yp)))\n",
    "    rmse = float(np.sqrt(np.mean((yt - yp) ** 2)))\n",
    "\n",
    "    denom = np.where(yt == 0, 1.0, yt)\n",
    "    mape = float(np.mean(np.abs((yt - yp) / denom)) * 100.0)\n",
    "\n",
    "    smape_denom = (np.abs(yt) + np.abs(yp))\n",
    "    smape_denom = np.where(smape_denom == 0, 1.0, smape_denom)\n",
    "    smape = float(np.mean(2.0 * np.abs(yt - yp) / smape_denom) * 100.0)\n",
    "\n",
    "    return {\"N\": int(mask.sum()), \"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape, \"sMAPE%\": smape}\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Collect predictions from Step 8 / 8c\n",
    "# -----------------------------\n",
    "preds = {}\n",
    "\n",
    "# ETS (8a)\n",
    "if \"ets_fc\" in globals():\n",
    "    preds[\"ETS\"] = _align_to_test(ets_fc, y_true.index)\n",
    "\n",
    "# Prophet (8b)\n",
    "if \"yhat\" in globals():\n",
    "    preds[\"Prophet\"] = _align_to_test(yhat, y_true.index)\n",
    "\n",
    "# 8c models (Linear/XGBoost/...)\n",
    "if \"results\" in globals() and isinstance(results, dict):\n",
    "    for name, pred in results.items():\n",
    "        try:\n",
    "            preds[str(name)] = _align_to_test(pred, y_true.index)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Preskacem model '{name}' (ne mogu da poravnam predikcije): {e}\")\n",
    "\n",
    "if not preds:\n",
    "    raise RuntimeError(\"Nema pronadjenih predikcija. Pokreni Section 8 i 8c pre Section 9 (ETS/Prophet + lag modeli).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build metrics table\n",
    "# -----------------------------\n",
    "rows = []\n",
    "for name, pred in preds.items():\n",
    "    m = _metrics(y_true, pred)\n",
    "    rows.append({\"Model\": name, **m})\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).set_index(\"Model\")\n",
    "metrics_df = metrics_df.sort_values(\"RMSE\", ascending=True)\n",
    "\n",
    "print(\"Model metrics (sorted by RMSE):\")\n",
    "display(metrics_df)\n",
    "\n",
    "best_model = metrics_df.index[0]\n",
    "print(f\"Best (lowest RMSE): {best_model}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Plotly overlay (train tail + test + all forecasts)\n",
    "# -----------------------------\n",
    "VIEW_LAST_TRAIN_DAYS = int(globals().get(\"VIEW_LAST_TRAIN_DAYS\", 200))\n",
    "train_view_start = max(_train_all.index.min(), _train_all.index.max() - pd.Timedelta(days=VIEW_LAST_TRAIN_DAYS - 1))\n",
    "train_view = _train_all.loc[train_view_start:, \"sales\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Train tail\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=train_view.index, y=train_view.values,\n",
    "    mode=\"lines\", name=f\"train (last {VIEW_LAST_TRAIN_DAYS}d)\"\n",
    "))\n",
    "\n",
    "# Test actual\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=y_true.index, y=y_true.values,\n",
    "    mode=\"lines\", name=\"test (actual)\"\n",
    "))\n",
    "\n",
    "# Vertical split marker\n",
    "fig.add_vline(x=y_true.index.min(), line_dash=\"dash\")\n",
    "\n",
    "# Forecasts\n",
    "for name, pred in preds.items():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pred.index, y=pred.values,\n",
    "        mode=\"lines\", name=f\"{name} forecast\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Model comparison — test forecast overlay (best RMSE: {best_model})\",\n",
    "    xaxis_title=\"date\",\n",
    "    yaxis_title=\"sales\",\n",
    "    hovermode=\"x unified\",\n",
    "    legend_title=\"series\",\n",
    "    height=520,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
